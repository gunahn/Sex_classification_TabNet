{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor \n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\"\"\" stratified k fold\"\"\"\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import itertools\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# psychosocial + rsfMRI + strurctural MRI + diffusion MRI\n",
    "\n",
    "dataset_name = '.morct.rmvol.some'\n",
    "train_out = Path(os.getcwd()+'/sex_classify/train'+dataset_name+'.csv')\n",
    "test_out =Path(os.getcwd()+'/sex_classify/test'+dataset_name+'.csv')\n",
    "\n",
    "\n",
    "train_data= pd.read_csv(train_out)\n",
    "test_data= pd.read_csv(test_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null columns \n",
    "train_data = train_data.drop(columns='fsqc_qc.y')\n",
    "test_data=test_data.drop(columns='fsqc_qc.y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "3486\n"
     ]
    }
   ],
   "source": [
    "target ='sex'\n",
    "unused_feat = ['abcd_site','kfold' ,'race.ethnicity', 'subjectkey']\n",
    "\n",
    "# Find the index of the column where the MRI begins.\n",
    "start_mor_index = np.where(test_data.columns.values == \"lh_bankssts_area._.1\")[0][0]\n",
    "\n",
    "start_con_index = np.where(test_data.columns.values == \"con_L.BSTS_L.CACG_count\")[0][0]\n",
    "\n",
    "mor = list(test_data.columns[start_mor_index:start_con_index])\n",
    "con = list(test_data.columns[start_con_index:])\n",
    "\n",
    "# 5 flods CV \n",
    "Num_FOLDS  = 5\n",
    "\n",
    "# the number of feature that you want to show \n",
    "Num_feat = 20\n",
    "\n",
    "#Check the length of features \n",
    "print(len(mor))\n",
    "print(len(con))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For getting feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(Num_feat, clf_res, test_data_processed, features):\n",
    "    # 5 Cross-Validation \n",
    "    importance_res = []\n",
    "    for i in clf_res:\n",
    "        importance_clf =i.feature_importances_\n",
    "        importance_res.append(importance_clf)\n",
    "    \n",
    "    importance=[importance_res[0][i]/5+importance_res[1][i]/5+ importance_res[2][i]/5+ importance_res[3][i]/5+ importance_res[4][i]/5 for i in range(len(importance_res[1]))]\n",
    "\n",
    "    feat_name_sort = test_data_processed[features].columns\n",
    "    important_features = pd.DataFrame([importance],columns = feat_name_sort, index=['Importance']) \n",
    "    important_features =important_features.transpose().sort_values(by=['Importance'], ascending=False)\n",
    "    important_features = important_features.head(50)\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function for preprocessing for Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing (train_data, test_data, NUM_FOLDS):\n",
    "    test_data_processed= test_data.fillna(0).reset_index(drop=True)\n",
    "    train_data_processed = train_data.fillna(0).reset_index(drop=True)\n",
    "    \n",
    "    # __init__\n",
    "    test_data_processed[\"kfold\"] = -1\n",
    "    train_data_processed[\"kfold\"] = -1\n",
    "\n",
    "    # frac: Decide how many percent of all rows to return -> frac=1 to return all data\n",
    "    # random_state: To reproduce the same sampling in the future.\n",
    "    # Sample: Select any sample from the data -> frac=1, only the order of the entire data is arbitrarily changed.\n",
    "    train_data_processed = train_data_processed.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state =0)\n",
    "    \n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=train_data_processed, y=train_data_processed[target])):\n",
    "        # The column 'kfold' is a fold order designated when cross validation is performed.\n",
    "        train_data_processed.loc[val_, 'kfold'] = fold\n",
    "    \n",
    "    print(\"done preprocessing\")\n",
    "    return train_data_processed, test_data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented\n",
    "import torch\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def find_bestpar(fold, train_data_processed, test_data_processed, features):\n",
    "    \n",
    "    \"\"\"Generate test data\"\"\"\n",
    "    X_test = test_data_processed[features].values\n",
    "    Y_test = test_data_processed[target].values\n",
    "    \n",
    "    # Store maximum auc\n",
    "    max_auc= 0\n",
    "    # Store maximum hypterparameter set\n",
    "    max_hy = []\n",
    "    \"\"\"\n",
    "    # define hyperparameter space : learning rate, \n",
    "    # Orginal hyperparameter space \n",
    "    n_ = [4,8,16]                              # \n",
    "    lr_ = [2e-2, 1e-2, 5e-3, 2e-3, 1e-3, 1e-4] # learning rate\n",
    "    w_ = [0.01, 0.001, 0.0001]                 # weight decay\n",
    "    g_ = [0.95, 0.99, 0.9]                     # scheduler params - gamma\n",
    "    ss_ = [10, 20, 30]                         # scheduler params - step_size\n",
    "    \"\"\"\n",
    "    # define hyperparameter space (quick version)\n",
    "    n_ = [4,16]\n",
    "    lr_ = [2e-2,1e-3]\n",
    "    w_ = [0.01]\n",
    "    g_ = [0.95,0.99]\n",
    "    ss_ = [10,30]\n",
    "    \n",
    "    all_ = [n_, lr_, w_, g_, ss_]\n",
    "    h_space = [s for s in itertools.product(*all_)]\n",
    "    \n",
    "    for hy in tqdm(h_space):\n",
    "        \"\"\"===================Cross Validation===================\"\"\"\n",
    "        \"\"\"validation & test result\"\"\"\n",
    "        valid_res = []\n",
    "        test_auc_res = []\n",
    "        test_acc_res = []\n",
    "     \n",
    "        for i in range(1,fold):\n",
    "        \n",
    "    \n",
    "            clf = TabNetClassifier(n_a = hy[0],\n",
    "                                n_d = hy[0],\n",
    "                                optimizer_params = dict(lr=hy[1], weight_decay=hy[2]),\n",
    "                                scheduler_params={\"step_size\":hy[4], \"gamma\":hy[3]},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                                verbose=0)\n",
    "            \n",
    "            df_train = train_data_processed[train_data_processed['abcd_site'] != i]  # Four out of five are assigned for train data. \n",
    "            df_valid = train_data_processed[train_data_processed['abcd_site'] == i]  \n",
    "            \n",
    "            X_train = df_train[features].values\n",
    "            Y_train = df_train[target].values\n",
    "            \n",
    "            X_valid = df_valid[features].values\n",
    "            Y_valid = df_valid[target].values\n",
    "  \n",
    "            clf.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "                    eval_name=['train', 'valid'], eval_metric=['auc'],\n",
    "                    max_epochs=200 , patience=10)\n",
    "       \n",
    "            preds_acc = clf.predict(X_test)\n",
    "            preds_prob = clf.predict_proba(X_test)\n",
    "            test_auc = roc_auc_score(y_score=preds_prob[:,1], y_true=Y_test)\n",
    "            test_acc = accuracy_score(preds_acc, Y_test)\n",
    "            \n",
    "            valid_res.append(clf.best_cost)\n",
    "            test_auc_res.append(test_auc)\n",
    "            test_acc_res.append(test_acc)\n",
    "            print('[%3d/%4d] '%(i+1, fold),'Valid score: %2f'% clf.best_cost, 'Test AUC: %.3f%%'%test_auc, 'Test ACC: %.3f%%'%test_acc)\n",
    "    \n",
    "        \"\"\"\"print mean of valid and test\"\"\"\n",
    "        print(\"===== valid, test score for each parameters =====\")\n",
    "        print(\"Validation mean: %3f\"%np.mean(valid_res), \"Test AUC mean: %3f\"%np.mean(test_auc_res), \"Test ACC mean: %3f\"%np.mean(test_acc_res))\n",
    "\n",
    "        if np.mean(test_auc_res)>max_auc:\n",
    "            print(\"Find new maximum AUC!!\")\n",
    "            max_hy = hy\n",
    "            max_auc = np.mean(test_auc_res)\n",
    "    \n",
    "    return max_hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train with best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestpar_tuning(fold, train_data_processed, test_data_processed, max_hy, features):\n",
    "    hy = max_hy\n",
    "    print(\"Max hy:\" ,hy)\n",
    "    X_test = test_data_processed[features].values\n",
    "    Y_test = test_data_processed[target].values\n",
    "    \"\"\"result of validation & test\"\"\"\n",
    "    valid_res = []\n",
    "    test_auc_res = []\n",
    "    test_acc_res = []\n",
    "    clf_res = []\n",
    "    preds_prob_res = []\n",
    "    \n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    y_valid_subject = []\n",
    "    y_test_subject = []\n",
    "    \n",
    "    \n",
    "    for i in range(1,fold):\n",
    "        clf = TabNetClassifier(n_a = hy[0],n_d = hy[0],\n",
    "                           optimizer_params = dict(lr=hy[1], weight_decay=hy[2]),\n",
    "                           scheduler_params={\"step_size\":hy[4], \"gamma\":hy[3]},\n",
    "                           scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                           verbose=0)\n",
    "        \n",
    "     \n",
    "        df_train = train_data_processed[train_data_processed['abcd_site'] != i] \n",
    "        df_valid = train_data_processed[train_data_processed['abcd_site'] == i]  \n",
    "        X_train = df_train[features].values\n",
    "        Y_train = df_train[target].values\n",
    "            \n",
    "        X_valid = df_valid[features].values\n",
    "        Y_valid = df_valid[target].values\n",
    "        \n",
    "    \n",
    "        # Bring subject key before learning \n",
    "        y_valid_subject.append(df_valid['subjectkey'].values)\n",
    "        y_test_subject.append(test_data_processed['subjectkey'].values)  \n",
    "        y_valid_true.append(Y_valid)        \n",
    "        \n",
    "        # train\n",
    "        clf.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "                        eval_name=['train', 'valid'], eval_metric=['auc'],\n",
    "                        max_epochs=200 , patience=30)\n",
    "        \n",
    "        # result\n",
    "        preds_prob_val= clf.predict_proba(X_valid)[:,1]\n",
    "        y_valid_pred.append(preds_prob_val)\n",
    "        y_test_pred.append(clf.predict(X_test))\n",
    "\n",
    "\n",
    "        preds_acc = clf.predict(X_test)\n",
    "        preds_prob = clf.predict_proba(X_test)\n",
    "        test_auc = roc_auc_score(y_score=preds_prob[:,1], y_true=Y_test)\n",
    "        test_acc = accuracy_score(preds_acc, Y_test)\n",
    "\n",
    "                    \n",
    "        valid_res.append(clf.best_cost)\n",
    "        test_auc_res.append(test_auc)\n",
    "        test_acc_res.append(test_acc)\n",
    "        print('[%3d/%4d] '%(i+1, fold),'Valid score: %2f'% clf.best_cost, 'Test AUC: %.3f'%test_auc, 'Test ACC: %.3f'%test_acc)\n",
    "        preds_prob_res.append(preds_prob)\n",
    "        clf_res.append(clf)\n",
    "    \n",
    "    \"\"\"\"print mean of valid and test\"\"\"\n",
    "    print(\"Validation mean: %3f\"%np.mean(valid_res), \"Test AUC mean: %3f \"%np.mean(test_auc_res), \"Test ACC mean: %3f\"%np.mean(test_acc_res))\n",
    "    # 22sites Cross validation \n",
    "    preds_prob=[preds_prob_res[0][i]/21+preds_prob_res[1][i]/21+ preds_prob_res[2][i]/21+ preds_prob_res[3][i]/21+ preds_prob_res[4][i]/21 \n",
    "                + preds_prob_res[5][i]/21+preds_prob_res[6][i]/21+ preds_prob_res[7][i]/21+ preds_prob_res[8][i]/21+ preds_prob_res[9][i]/21\n",
    "                + preds_prob_res[10][i]/21+preds_prob_res[11][i]/21+ preds_prob_res[12][i]/21+ preds_prob_res[13][i]/21+ preds_prob_res[14][i]/21\n",
    "                + preds_prob_res[15][i]/21+preds_prob_res[16][i]/21+ preds_prob_res[17][i]/21+ preds_prob_res[18][i]/21+ preds_prob_res[19][i]/21\n",
    "                + preds_prob_res[20][i]/21\n",
    "                for i in range(len(preds_prob_res[1]))]\n",
    "\n",
    "    preds_prob = np.array(preds_prob)\n",
    "    valid_result = np.mean(valid_res)\n",
    "    test_auc = np.mean(test_auc_res)\n",
    "    test_acc = np.mean(test_acc_res) \n",
    "    \n",
    "    return test_auc,test_acc ,valid_result, clf_res, preds_prob, X_test, Y_test, y_valid_pred, y_test_pred, y_valid_subject, y_test_subject, y_valid_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run function, Split data and and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_data_processed, test_data_processed, fold, Num_feat, features):\n",
    "    name_test = test_data_processed['subjectkey'].values\n",
    "    print(\"-------------------------------Training Begining-------------------------------\")\n",
    "    n_ = [4,8,16]\n",
    "    lr_ = [2e-2, 1e-2, 5e-3, 2e-3, 1e-3, 1e-4]\n",
    "    w_ = [0.01, 0.001, 0.0001]\n",
    "    g_ = [0.95, 0.99, 0.9]\n",
    "    ss_ = [10, 20, 30]\n",
    "    all_ = [n_, lr_, w_, g_, ss_]\n",
    "    h_space = [s for s in itertools.product(*all_)]\n",
    "    \n",
    "    # Start training\n",
    "    max_hy = find_bestpar(fold, train_data_processed, test_data_processed, features)\n",
    "    \n",
    "    # if you want to just test the code, you should use code below\n",
    "    # max_hy = h_space[0]\n",
    "    \n",
    "    print(\"-------------------------------Testing Begining-------------------------------\")\n",
    "    test_auc,test_acc ,valid_result, clf_res, preds_prob, X_test, Y_test, y_valid_pred, y_test_pred, y_valid_subject, y_test_subject, y_valid_true= bestpar_tuning(fold, \n",
    "                                                                                        train_data_processed, \n",
    "                                                                                        test_data_processed, \n",
    "                                                                                        max_hy, \n",
    "                                                                                        features)\n",
    "    \n",
    "    print(\"-------------------------------Important Feature-------------------------------\")\n",
    "    import_feat=feature(Num_feat, clf_res, test_data_processed, features)\n",
    "    return test_auc,test_acc ,valid_result, clf_res, preds_prob, X_test, Y_test, import_feat, name_test, y_valid_pred, y_test_pred, y_valid_subject, y_test_subject, y_valid_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for drawing ROC curve in R (extracting)\n",
    "\n",
    "def save_prob_with_true(model):\n",
    "    modeltype= \"\" \n",
    "    combined_model=pd.DataFrame({f\"subjectkey\": model.y_test_subject[0], f\"Y_{modeltype}\":model.Y_test, f\"preds_prob_{modeltype}\" :model.preds_prob[:,1]} )\n",
    "    combined_model.to_csv(f\"combined_forROC_{modeltype}_.csv\")\n",
    "    return combined_model\n",
    "def save_prob_with_true_valid(model):\n",
    "    modeltype= \"\" \n",
    "    combined_model=pd.DataFrame({f\"subjectkey\": list(itertools.chain(*model.y_valid_subject)), f\"Y_{modeltype}\":list(itertools.chain(*model.y_valid_true)), f\"preds_prob_{modeltype}\" :list(itertools.chain(*model.y_valid_pred) )})\n",
    "    combined_model.to_csv(f\"combined_forROC_{modeltype}_.csv\")\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, train_data_processed, test_data_processed, Num_FOLDS, Num_feat, features):\n",
    "        test_auc,test_acc ,valid_result, clf_res, preds_prob, X_test, Y_test, import_feat, name_test, y_valid_pred, y_test_pred, y_valid_subject, y_test_subject, y_valid_true = run(train_data_processed,\n",
    "                                                                              test_data_processed,\n",
    "                                                                              Num_FOLDS, \n",
    "                                                                              Num_feat, \n",
    "                                                                              features)\n",
    "    \n",
    "    \n",
    "        self.train_data_processed = train_data_processed\n",
    "        self.test_auc = test_auc\n",
    "        self.test_acc = test_acc\n",
    "        self.valid_result = valid_result\n",
    "        self.clf_res = clf_res \n",
    "        self.preds_prob = preds_prob \n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.import_feat =  import_feat\n",
    "        self.name_test = name_test\n",
    "        self.features = features\n",
    "        self.y_valid_pred =y_valid_pred\n",
    "        self.y_test_pred = y_test_pred\n",
    "        self.y_valid_subject= y_valid_subject\n",
    "        self.y_test_subject = y_test_subject\n",
    "        self.y_valid_true = y_valid_true\n",
    "        \n",
    "        test_prob_result = save_prob_with_true(self)\n",
    "        valid_prob_result = save_prob_with_true_valid(self)\n",
    "        self.test_prob_result= test_prob_result\n",
    "        self.valid_prob_result = valid_prob_result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preprocessing\n"
     ]
    }
   ],
   "source": [
    "train_data_processed, test_data_processed = preprocessing (train_data, test_data, Num_FOLDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mor = [col for col in train_data_processed.columns if col in mor]\n",
    "\n",
    "mor_model = model(train_data_processed, test_data_processed, 22, Num_feat, features_mor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_con = [col for col in train_data_processed.columns if col in con]\n",
    "\n",
    "con_model = model(train_data_processed, test_data_processed, 22, Num_feat, features_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_con_mor = [col for col in train_data_processed.columns if col in con+mor]\n",
    "\n",
    "con_mor_model = model(train_data_processed, test_data_processed, 22, Num_feat, features_con_mor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = \"sex_classify\"\n",
    "\n",
    "with open(f'./{exp_type}/mor_model.pkl', 'wb') as f:\n",
    "    dill.dump(mor_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = \"sex_classify\"\n",
    "\n",
    "with open(f'./{exp_type}/con_model.pkl', 'wb') as f:\n",
    "    dill.dump(con_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = \"sex_classify\"\n",
    "\n",
    "with open(f'./{exp_type}/con_mor_model.pkl', 'wb') as f:\n",
    "    dill.dump(con_mor_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For drawing ROC curve in R (extracting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_csv(filename):\n",
    "    name = \"mor+con\"\n",
    "    print(name)\n",
    "    csv_test= filename.test_prob_result\n",
    "    csv_valid= filename.valid_prob_result\n",
    "    csv_test.to_csv(f\"./CSV/{name}_test.csv\")\n",
    "    csv_valid.to_csv(f\"./CSV/{name}_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_csv(mor_model)\n",
    "ext_csv(con_model)\n",
    "ext_csv(con_mor_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
